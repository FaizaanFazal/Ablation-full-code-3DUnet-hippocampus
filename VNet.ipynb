{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ee6877",
   "metadata": {},
   "source": [
    "#### Alternate Backbone Check VNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397641d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.12.1+cu113\n",
      "CUDA: 11.3\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1958e",
   "metadata": {},
   "source": [
    "#### VNet Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa463d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in d:\\faizaan\\1_data_projects\\projects\\segmentationunet3d\\ai_new_env\\lib\\site-packages (1.8.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VNet                                     [1, 1, 64, 64, 96]        --\n",
       "├─VNetBlock: 1-1                         [1, 16, 64, 64, 96]       --\n",
       "│    └─Conv3d: 2-1                       [1, 16, 64, 64, 96]       448\n",
       "│    └─InstanceNorm3d: 2-2               [1, 16, 64, 64, 96]       --\n",
       "│    └─ReLU: 2-3                         [1, 16, 64, 64, 96]       --\n",
       "│    └─Conv3d: 2-4                       [1, 16, 64, 64, 96]       6,928\n",
       "│    └─InstanceNorm3d: 2-5               [1, 16, 64, 64, 96]       --\n",
       "│    └─ReLU: 2-6                         [1, 16, 64, 64, 96]       --\n",
       "├─MaxPool3d: 1-2                         [1, 16, 32, 32, 48]       --\n",
       "├─VNetBlock: 1-3                         [1, 32, 32, 32, 48]       --\n",
       "│    └─Conv3d: 2-7                       [1, 32, 32, 32, 48]       13,856\n",
       "│    └─InstanceNorm3d: 2-8               [1, 32, 32, 32, 48]       --\n",
       "│    └─ReLU: 2-9                         [1, 32, 32, 32, 48]       --\n",
       "│    └─Conv3d: 2-10                      [1, 32, 32, 32, 48]       27,680\n",
       "│    └─InstanceNorm3d: 2-11              [1, 32, 32, 32, 48]       --\n",
       "│    └─ReLU: 2-12                        [1, 32, 32, 32, 48]       --\n",
       "├─MaxPool3d: 1-4                         [1, 32, 16, 16, 24]       --\n",
       "├─VNetBlock: 1-5                         [1, 64, 16, 16, 24]       --\n",
       "│    └─Conv3d: 2-13                      [1, 64, 16, 16, 24]       55,360\n",
       "│    └─InstanceNorm3d: 2-14              [1, 64, 16, 16, 24]       --\n",
       "│    └─ReLU: 2-15                        [1, 64, 16, 16, 24]       --\n",
       "│    └─Conv3d: 2-16                      [1, 64, 16, 16, 24]       110,656\n",
       "│    └─InstanceNorm3d: 2-17              [1, 64, 16, 16, 24]       --\n",
       "│    └─ReLU: 2-18                        [1, 64, 16, 16, 24]       --\n",
       "├─MaxPool3d: 1-6                         [1, 64, 8, 8, 12]         --\n",
       "├─VNetBlock: 1-7                         [1, 128, 8, 8, 12]        --\n",
       "│    └─Conv3d: 2-19                      [1, 128, 8, 8, 12]        221,312\n",
       "│    └─InstanceNorm3d: 2-20              [1, 128, 8, 8, 12]        --\n",
       "│    └─ReLU: 2-21                        [1, 128, 8, 8, 12]        --\n",
       "│    └─Conv3d: 2-22                      [1, 128, 8, 8, 12]        442,496\n",
       "│    └─InstanceNorm3d: 2-23              [1, 128, 8, 8, 12]        --\n",
       "│    └─ReLU: 2-24                        [1, 128, 8, 8, 12]        --\n",
       "├─ConvTranspose3d: 1-8                   [1, 64, 16, 16, 24]       65,600\n",
       "├─VNetBlock: 1-9                         [1, 64, 16, 16, 24]       --\n",
       "│    └─Conv3d: 2-25                      [1, 64, 16, 16, 24]       221,248\n",
       "│    └─InstanceNorm3d: 2-26              [1, 64, 16, 16, 24]       --\n",
       "│    └─ReLU: 2-27                        [1, 64, 16, 16, 24]       --\n",
       "│    └─Conv3d: 2-28                      [1, 64, 16, 16, 24]       110,656\n",
       "│    └─InstanceNorm3d: 2-29              [1, 64, 16, 16, 24]       --\n",
       "│    └─ReLU: 2-30                        [1, 64, 16, 16, 24]       --\n",
       "├─ConvTranspose3d: 1-10                  [1, 32, 32, 32, 48]       16,416\n",
       "├─VNetBlock: 1-11                        [1, 32, 32, 32, 48]       --\n",
       "│    └─Conv3d: 2-31                      [1, 32, 32, 32, 48]       55,328\n",
       "│    └─InstanceNorm3d: 2-32              [1, 32, 32, 32, 48]       --\n",
       "│    └─ReLU: 2-33                        [1, 32, 32, 32, 48]       --\n",
       "│    └─Conv3d: 2-34                      [1, 32, 32, 32, 48]       27,680\n",
       "│    └─InstanceNorm3d: 2-35              [1, 32, 32, 32, 48]       --\n",
       "│    └─ReLU: 2-36                        [1, 32, 32, 32, 48]       --\n",
       "├─ConvTranspose3d: 1-12                  [1, 16, 64, 64, 96]       4,112\n",
       "├─VNetBlock: 1-13                        [1, 16, 64, 64, 96]       --\n",
       "│    └─Conv3d: 2-37                      [1, 16, 64, 64, 96]       13,840\n",
       "│    └─InstanceNorm3d: 2-38              [1, 16, 64, 64, 96]       --\n",
       "│    └─ReLU: 2-39                        [1, 16, 64, 64, 96]       --\n",
       "│    └─Conv3d: 2-40                      [1, 16, 64, 64, 96]       6,928\n",
       "│    └─InstanceNorm3d: 2-41              [1, 16, 64, 64, 96]       --\n",
       "│    └─ReLU: 2-42                        [1, 16, 64, 64, 96]       --\n",
       "├─Conv3d: 1-14                           [1, 1, 64, 64, 96]        17\n",
       "==========================================================================================\n",
       "Total params: 1,400,561\n",
       "Trainable params: 1,400,561\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 23.59\n",
       "==========================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 335.02\n",
       "Params size (MB): 5.60\n",
       "Estimated Total Size (MB): 342.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, 3, padding=1)\n",
    "        self.in1 = nn.InstanceNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, 3, padding=1)\n",
    "        self.in2 = nn.InstanceNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.in1(self.conv1(x)))\n",
    "        x = self.relu(self.in2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "# ...existing code...\n",
    "# UPDATE: Remove sigmoid from VNet forward (return raw logits)\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=16):\n",
    "        super().__init__()\n",
    "        c1, c2, c3, c4 = base_channels, base_channels*2, base_channels*4, base_channels*8\n",
    "        self.enc1 = VNetBlock(in_channels, c1)\n",
    "        self.pool1 = nn.MaxPool3d(2)\n",
    "        self.enc2 = VNetBlock(c1, c2)\n",
    "        self.pool2 = nn.MaxPool3d(2)\n",
    "        self.enc3 = VNetBlock(c2, c3)\n",
    "        self.pool3 = nn.MaxPool3d(2)\n",
    "        self.bottleneck = VNetBlock(c3, c4)\n",
    "        self.up3 = nn.ConvTranspose3d(c4, c3, 2, stride=2)\n",
    "        self.dec3 = VNetBlock(c3 + c3, c3)\n",
    "        self.up2 = nn.ConvTranspose3d(c3, c2, 2, stride=2)\n",
    "        self.dec2 = VNetBlock(c2 + c2, c2)\n",
    "        self.up1 = nn.ConvTranspose3d(c2, c1, 2, stride=2)\n",
    "        self.dec1 = VNetBlock(c1 + c1, c1)\n",
    "        self.final = nn.Conv3d(c1, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b  = self.bottleneck(self.pool3(e3))\n",
    "        d3 = self.up3(b)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "        return self.final(d1)  \n",
    "    \n",
    "\n",
    "%pip install torchinfo\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sampleModel = VNet(in_channels=1, out_channels=1).to(device)\n",
    "\n",
    "# N, C, D, H, W = 1, 1, 64, 64, 96\n",
    "summary(sampleModel, input_size=(1, 1, 64, 64, 96), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824df01",
   "metadata": {},
   "source": [
    "##### Resuable functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf69cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "def under_segmentation_rate(y_true, y_pred, smooth=1):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    y_true_labels = (y_true >= 0.5).float()\n",
    "    false_neg = torch.sum(y_true_labels * (1 - y_pred_labels))\n",
    "    ground_truth_area = torch.sum(y_true_labels)\n",
    "    prediction_area = torch.sum(y_pred_labels)\n",
    "    return (false_neg + smooth) / (ground_truth_area + prediction_area + smooth)\n",
    "\n",
    "def over_segmentation_ratio(y_true, y_pred, smooth=1e-5):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    y_true_labels = (y_true >= 0.5).float()\n",
    "    false_pos = torch.sum((y_pred_labels == 1) & (y_true_labels == 0))\n",
    "    ground_truth_area = torch.sum(y_true_labels)\n",
    "    prediction_area = torch.sum(y_pred_labels)\n",
    "    return (false_pos + smooth) / (ground_truth_area + prediction_area + smooth)\n",
    "\n",
    "def average_surface_distance(y_true, y_pred, spacing=None):\n",
    "    y_true_np = y_true.detach().cpu().numpy().astype(np.bool_)\n",
    "    y_pred_np = y_pred.detach().cpu().numpy().astype(np.bool_)\n",
    "    conn = morphology.generate_binary_structure(y_true_np.ndim, 1)\n",
    "    true_surface = np.logical_xor(y_true_np, morphology.binary_erosion(y_true_np, conn))\n",
    "    pred_surface = np.logical_xor(y_pred_np, morphology.binary_erosion(y_pred_np, conn))\n",
    "    if np.sum(true_surface) == 0 or np.sum(pred_surface) == 0:\n",
    "        return float('inf')\n",
    "    true_distances = morphology.distance_transform_edt(~true_surface, sampling=spacing)\n",
    "    pred_distances = morphology.distance_transform_edt(~pred_surface, sampling=spacing)\n",
    "    dist_pred_to_true = np.mean(true_distances[pred_surface != 0])\n",
    "    dist_true_to_pred = np.mean(pred_distances[true_surface != 0])\n",
    "    return (dist_pred_to_true + dist_true_to_pred) / 2.0\n",
    "\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    y_true_labels = (y_true >= 0.5).cpu().numpy().astype(np.bool_)\n",
    "    y_pred_labels = (y_pred >= 0.5).cpu().numpy().astype(np.bool_)\n",
    "    distances = []\n",
    "    for slice_idx in range(y_true_labels.shape[0]):\n",
    "        ts = y_true_labels[slice_idx, :, :]\n",
    "        ps = y_pred_labels[slice_idx, :, :]\n",
    "        if np.sum(ts) == 0 or np.sum(ps) == 0:\n",
    "            continue\n",
    "        true_pts = np.argwhere(ts)\n",
    "        pred_pts = np.argwhere(ps)\n",
    "        hd_f = directed_hausdorff(true_pts, pred_pts)[0]\n",
    "        hd_b = directed_hausdorff(pred_pts, true_pts)[0]\n",
    "        distances.append(max(hd_f, hd_b))\n",
    "    return np.mean(distances) if distances else float('inf')\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    inter = torch.sum(y_true * y_pred_labels)\n",
    "    return (2 * inter + smooth) / (torch.sum(y_true) + torch.sum(y_pred_labels) + smooth)\n",
    "\n",
    "def jaccard_index(y_true, y_pred, smooth=1):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    inter = torch.sum(y_true * y_pred_labels)\n",
    "    union = torch.sum(y_true) + torch.sum(y_pred_labels) - inter\n",
    "    return (inter + smooth) / (union + smooth)\n",
    "\n",
    "def mean_iou(y_true, y_pred, smooth=1):\n",
    "    return jaccard_index(y_true, y_pred, smooth)\n",
    "\n",
    "def f1_score(y_true, y_pred, smooth=1):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    tp = torch.sum(y_true * y_pred_labels)\n",
    "    precision = tp / (torch.sum(y_pred_labels) + smooth)\n",
    "    recall = tp / (torch.sum(y_true) + smooth)\n",
    "    return (2 * precision * recall + smooth) / (precision + recall + smooth)\n",
    "\n",
    "def recall_score(y_true, y_pred, smooth=1):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    tp = torch.sum(y_true * y_pred_labels)\n",
    "    return (tp + smooth) / (torch.sum(y_true) + smooth)\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    y_pred_labels = (y_pred >= 0.5).float()\n",
    "    return (y_pred_labels == y_true).float().sum() / y_true.numel()\n",
    "\n",
    "# Order chosen to ensure desired printed metrics appear first\n",
    "METRIC_FUNCS = {\n",
    "    \"dice\": dice_coefficient,\n",
    "    \"f1\": f1_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"jaccard\": jaccard_index,\n",
    "    \"hausdorff\": hausdorff_distance,\n",
    "    \"usr\": under_segmentation_rate,\n",
    "    \"osr\": over_segmentation_ratio,\n",
    "    \"asd\": average_surface_distance,\n",
    "    \"accuracy\": calculate_accuracy,\n",
    "    \"mean_iou\": mean_iou\n",
    "}\n",
    "\n",
    "class MetricAggregator:\n",
    "    def __init__(self, metric_funcs=None):\n",
    "        self.metric_funcs = metric_funcs or METRIC_FUNCS\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.storage = {k: [] for k in self.metric_funcs.keys()}\n",
    "    def update(self, y_true, y_pred):\n",
    "        if y_true.dim() == 5:\n",
    "            for b in range(y_true.size(0)):\n",
    "                self._update_single(y_true[b, 0], y_pred[b, 0])\n",
    "        elif y_true.dim() == 4:\n",
    "            for b in range(y_true.size(0)):\n",
    "                self._update_single(y_true[b], y_pred[b])\n",
    "        else:\n",
    "            self._update_single(y_true, y_pred)\n",
    "    def _update_single(self, yt, yp):\n",
    "        for name, fn in self.metric_funcs.items():\n",
    "            try:\n",
    "                val = fn(yt, yp)\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    val = val.item()\n",
    "            except Exception:\n",
    "                val = float('nan')\n",
    "            self.storage[name].append(val)\n",
    "    def compute(self):\n",
    "        out = {}\n",
    "        for k, arr in self.storage.items():\n",
    "            arr = [a for a in arr if a == a]\n",
    "            out[k] = float(np.mean(arr)) if len(arr) else 0.0\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93d041",
   "metadata": {},
   "source": [
    "##### data preparation, splitting, saving , checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb2e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "DATA_CONFIG = {\n",
    "    \"HARP_NP\": {\n",
    "        \"img_dir\": r\"D:\\Faizaan\\AlzhimersData\\ADNI\\HippoCampus_labels_data\\Task520_HarP\\imageTr\",\n",
    "        \"lbl_dir\": r\"D:\\Faizaan\\AlzhimersData\\ADNI\\HippoCampus_labels_data\\Task520_HarP\\labelTr\"\n",
    "    },\n",
    "    \"HARP_FP\": {\n",
    "        \"img_dir\": r\"D:\\Faizaan\\AlzhimersData\\ADNI\\HippoCampus_labels_data\\Task520_Harp_Preprocessed\",\n",
    "        \"lbl_dir\": r\"D:\\Faizaan\\AlzhimersData\\ADNI\\HippoCampus_labels_data\\Task520_HarP\\labelTr\"\n",
    "    }\n",
    "}\n",
    "\n",
    "class HarPDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, normalize=True):\n",
    "        self.img_paths = sorted([\n",
    "            os.path.join(img_dir, f) for f in os.listdir(img_dir)\n",
    "            if f.lower().endswith((\".nii\", \".nii.gz\"))\n",
    "        ])\n",
    "        self.lbl_paths = sorted([\n",
    "            os.path.join(label_dir, f) for f in os.listdir(label_dir)\n",
    "            if f.lower().endswith((\".nii\", \".nii.gz\"))\n",
    "        ])\n",
    "        assert len(self.img_paths) == len(self.lbl_paths), \"Image/label count mismatch\"\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.img_paths[idx]).get_fdata().astype(np.float32)\n",
    "        lbl = nib.load(self.lbl_paths[idx]).get_fdata().astype(np.float32)\n",
    "\n",
    "        if self.normalize:\n",
    "            m, M = img.min(), img.max()\n",
    "            if M > m:\n",
    "                img = (img - m) / (M - m)\n",
    "\n",
    "        # Binarize label (in case of multi-values)\n",
    "        lbl = (lbl > 0).astype(np.float32)\n",
    "\n",
    "        img = np.expand_dims(img, 0)\n",
    "        lbl = np.expand_dims(lbl, 0)\n",
    "\n",
    "        return torch.from_numpy(img), torch.from_numpy(lbl)\n",
    "\n",
    "def get_dataloaders(dataset_key: str,\n",
    "                    batch_size: int = 2,\n",
    "                    val_ratio: float = 0.1,\n",
    "                    test_ratio: float = 0.1,\n",
    "                    seed: int = 42) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    cfg = DATA_CONFIG[dataset_key]\n",
    "    ds = HarPDataset(cfg[\"img_dir\"], cfg[\"lbl_dir\"])\n",
    "    n = len(ds)\n",
    "    val_size = int(n * val_ratio)\n",
    "    test_size = int(n * test_ratio)\n",
    "    train_size = n - val_size - test_size\n",
    "    torch.manual_seed(seed)\n",
    "    train_ds, val_ds, test_ds = random_split(ds, [train_size, val_size, test_size])\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "        DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0),\n",
    "        DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798eb1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DATASET_KEY = \"HARP_NP\"  # change to \"HARP_FP\" later for preprocessed\n",
    "MODEL_TAG = \"NP\" if DATASET_KEY.endswith(\"NP\") else \"FP\"\n",
    "MODEL_NAME_BASE = f\"VNet_{MODEL_TAG}\"\n",
    "OUTPUT_ROOT = Path(\"VNETModels\") / DATASET_KEY\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_ROOT / \"checkpoints\").mkdir(exist_ok=True)\n",
    "CONFIG = {\n",
    "    \"dataset\": DATASET_KEY,\n",
    "    \"model_tag\": MODEL_TAG,\n",
    "    \"model_name\": MODEL_NAME_BASE,\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 250,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"loss\": \"BCE\",\n",
    "    \"threshold\": 0.5,\n",
    "    \"early_stopping_patience\":7,\n",
    "    \"mixed_precision\": True\n",
    "}\n",
    "with open(OUTPUT_ROOT / \"config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6c93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(name=\"BCE\"):\n",
    "    if name.upper() == \"BCE\":\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "    elif name.upper() == \"DICEBCE\":\n",
    "        class DiceBCELoss(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.bce = nn.BCEWithLogitsLoss()\n",
    "            def forward(self, pred_logits, target, smooth=1.):\n",
    "                bce = self.bce(pred_logits, target)\n",
    "                probs = torch.sigmoid(pred_logits)\n",
    "                p = probs.view(-1)\n",
    "                t = target.view(-1)\n",
    "                inter = (p * t).sum()\n",
    "                dice = (2*inter + smooth)/(p.sum() + t.sum() + smooth)\n",
    "                return bce + (1 - dice)\n",
    "        return DiceBCELoss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss\")\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best=False):\n",
    "    if not is_best:\n",
    "        return  # only act when a new best is found\n",
    "    tag = CONFIG[\"model_tag\"]\n",
    "    best_path = OUTPUT_ROOT / f\"best_model_{tag}.pth\"\n",
    "    torch.save(state, best_path)\n",
    "    (OUTPUT_ROOT / f\"trained_{tag}.flag\").write_text(str(state[\"epoch\"]))\n",
    "\n",
    "def save_history(history_rows, header, path):\n",
    "    import csv\n",
    "    write_header = not path.exists()\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow(header)\n",
    "        w.writerow(history_rows)\n",
    "\n",
    "def binarize(pred, thr):\n",
    "    return (pred >= thr).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89580ee7",
   "metadata": {},
   "source": [
    "##### training, testing (dataset key was HARP_NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0462e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 55 Val: 13 Test: 13\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    CONFIG[\"dataset\"],\n",
    "    batch_size=CONFIG[\"batch_size\"]\n",
    ")\n",
    "\n",
    "model = VNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion = get_loss(CONFIG[\"loss\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "scaler = GradScaler(enabled=CONFIG[\"mixed_precision\"])\n",
    "\n",
    "print(\"Train batches:\", len(train_loader), \"Val:\", len(val_loader), \"Test:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bca9baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:25: DeprecationWarning: Please import `generate_binary_structure` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  conn = morphology.generate_binary_structure(y_true_np.ndim, 1)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:26: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_surface = np.logical_xor(y_true_np, morphology.binary_erosion(y_true_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:27: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_surface = np.logical_xor(y_pred_np, morphology.binary_erosion(y_pred_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:30: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_distances = morphology.distance_transform_edt(~true_surface, sampling=spacing)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:31: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_distances = morphology.distance_transform_edt(~pred_surface, sampling=spacing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | TrainLoss 0.5740 | ValLoss 0.4799 | TrainAcc 0.8290 | ValAcc 0.9731 | dice 0.4590 | f1 0.7064 | recall 0.8758 | jaccard 0.2989 | hausdorff 40.3212 | usr 0.0350 | osr 0.5062 | BestDice 0.4590 | Pat 0 | 21.6s\n",
      "Epoch 002 | TrainLoss 0.4247 | ValLoss 0.3782 | TrainAcc 0.9853 | ValAcc 0.9923 | dice 0.7408 | f1 0.8451 | recall 0.8620 | jaccard 0.5916 | hausdorff 14.0804 | usr 0.0595 | osr 0.1998 | BestDice 0.7408 | Pat 0 | 18.2s\n",
      "Epoch 003 | TrainLoss 0.3368 | ValLoss 0.2942 | TrainAcc 0.9926 | ValAcc 0.9908 | dice 0.7245 | f1 0.8339 | recall 0.9383 | jaccard 0.5707 | hausdorff 7.3961 | usr 0.0241 | osr 0.2515 | BestDice 0.7408 | Pat 1 | 18.5s\n",
      "Epoch 004 | TrainLoss 0.2565 | ValLoss 0.2214 | TrainAcc 0.9933 | ValAcc 0.9953 | dice 0.8196 | f1 0.8880 | recall 0.8128 | jaccard 0.6952 | hausdorff 7.0300 | usr 0.0949 | osr 0.0856 | BestDice 0.8196 | Pat 0 | 19.0s\n",
      "Epoch 005 | TrainLoss 0.1982 | ValLoss 0.1751 | TrainAcc 0.9944 | ValAcc 0.9953 | dice 0.8292 | f1 0.8934 | recall 0.8898 | jaccard 0.7097 | hausdorff 5.7874 | usr 0.0518 | osr 0.1191 | BestDice 0.8292 | Pat 0 | 21.2s\n",
      "Epoch 006 | TrainLoss 0.1579 | ValLoss 0.1402 | TrainAcc 0.9949 | ValAcc 0.9958 | dice 0.8349 | f1 0.8967 | recall 0.8232 | jaccard 0.7174 | hausdorff 6.7497 | usr 0.0905 | osr 0.0747 | BestDice 0.8349 | Pat 0 | 24.1s\n",
      "Epoch 007 | TrainLoss 0.1282 | ValLoss 0.1150 | TrainAcc 0.9952 | ValAcc 0.9962 | dice 0.8536 | f1 0.9077 | recall 0.8501 | jaccard 0.7454 | hausdorff 5.9829 | usr 0.0757 | osr 0.0709 | BestDice 0.8536 | Pat 0 | 24.5s\n",
      "Epoch 008 | TrainLoss 0.1074 | ValLoss 0.0975 | TrainAcc 0.9954 | ValAcc 0.9960 | dice 0.8548 | f1 0.9083 | recall 0.9063 | jaccard 0.7473 | hausdorff 4.6870 | usr 0.0447 | osr 0.1006 | BestDice 0.8548 | Pat 0 | 24.3s\n",
      "Epoch 009 | TrainLoss 0.0914 | ValLoss 0.0847 | TrainAcc 0.9957 | ValAcc 0.9960 | dice 0.8521 | f1 0.9067 | recall 0.8896 | jaccard 0.7430 | hausdorff 5.4071 | usr 0.0533 | osr 0.0948 | BestDice 0.8548 | Pat 1 | 24.6s\n",
      "Epoch 010 | TrainLoss 0.0810 | ValLoss 0.0747 | TrainAcc 0.9954 | ValAcc 0.9956 | dice 0.8399 | f1 0.8996 | recall 0.8974 | jaccard 0.7254 | hausdorff 5.2599 | usr 0.0484 | osr 0.1118 | BestDice 0.8548 | Pat 2 | 24.6s\n",
      "Epoch 011 | TrainLoss 0.0712 | ValLoss 0.0651 | TrainAcc 0.9955 | ValAcc 0.9965 | dice 0.8659 | f1 0.9150 | recall 0.8682 | jaccard 0.7642 | hausdorff 5.2658 | usr 0.0661 | osr 0.0682 | BestDice 0.8659 | Pat 0 | 24.5s\n",
      "Epoch 012 | TrainLoss 0.0630 | ValLoss 0.0582 | TrainAcc 0.9959 | ValAcc 0.9966 | dice 0.8688 | f1 0.9167 | recall 0.8555 | jaccard 0.7686 | hausdorff 5.1166 | usr 0.0739 | osr 0.0575 | BestDice 0.8688 | Pat 0 | 24.7s\n",
      "Epoch 013 | TrainLoss 0.0570 | ValLoss 0.0535 | TrainAcc 0.9959 | ValAcc 0.9964 | dice 0.8573 | f1 0.9099 | recall 0.8350 | jaccard 0.7509 | hausdorff 6.3122 | usr 0.0851 | osr 0.0578 | BestDice 0.8688 | Pat 1 | 24.5s\n",
      "Epoch 014 | TrainLoss 0.0516 | ValLoss 0.0479 | TrainAcc 0.9960 | ValAcc 0.9967 | dice 0.8693 | f1 0.9170 | recall 0.8551 | jaccard 0.7695 | hausdorff 5.3402 | usr 0.0740 | osr 0.0568 | BestDice 0.8693 | Pat 0 | 24.4s\n",
      "Epoch 015 | TrainLoss 0.0471 | ValLoss 0.0442 | TrainAcc 0.9962 | ValAcc 0.9966 | dice 0.8676 | f1 0.9160 | recall 0.8738 | jaccard 0.7669 | hausdorff 5.0680 | usr 0.0631 | osr 0.0695 | BestDice 0.8693 | Pat 1 | 24.4s\n",
      "Epoch 016 | TrainLoss 0.0435 | ValLoss 0.0410 | TrainAcc 0.9962 | ValAcc 0.9964 | dice 0.8682 | f1 0.9163 | recall 0.9136 | jaccard 0.7680 | hausdorff 4.1330 | usr 0.0413 | osr 0.0907 | BestDice 0.8693 | Pat 2 | 24.5s\n",
      "Epoch 017 | TrainLoss 0.0404 | ValLoss 0.0378 | TrainAcc 0.9962 | ValAcc 0.9967 | dice 0.8763 | f1 0.9212 | recall 0.8910 | jaccard 0.7804 | hausdorff 3.5447 | usr 0.0540 | osr 0.0698 | BestDice 0.8763 | Pat 0 | 24.6s\n",
      "Epoch 018 | TrainLoss 0.0387 | ValLoss 0.0357 | TrainAcc 0.9958 | ValAcc 0.9966 | dice 0.8680 | f1 0.9163 | recall 0.8773 | jaccard 0.7677 | hausdorff 4.3505 | usr 0.0611 | osr 0.0709 | BestDice 0.8763 | Pat 1 | 24.4s\n",
      "Epoch 019 | TrainLoss 0.0353 | ValLoss 0.0326 | TrainAcc 0.9962 | ValAcc 0.9969 | dice 0.8798 | f1 0.9233 | recall 0.8770 | jaccard 0.7858 | hausdorff 4.3971 | usr 0.0620 | osr 0.0583 | BestDice 0.8798 | Pat 0 | 24.4s\n",
      "Epoch 020 | TrainLoss 0.0327 | ValLoss 0.0306 | TrainAcc 0.9964 | ValAcc 0.9969 | dice 0.8811 | f1 0.9241 | recall 0.8975 | jaccard 0.7881 | hausdorff 4.3656 | usr 0.0507 | osr 0.0683 | BestDice 0.8811 | Pat 0 | 24.4s\n",
      "Epoch 021 | TrainLoss 0.0311 | ValLoss 0.0291 | TrainAcc 0.9963 | ValAcc 0.9968 | dice 0.8779 | f1 0.9222 | recall 0.8870 | jaccard 0.7829 | hausdorff 4.2992 | usr 0.0564 | osr 0.0659 | BestDice 0.8811 | Pat 1 | 24.5s\n",
      "Epoch 022 | TrainLoss 0.0294 | ValLoss 0.0276 | TrainAcc 0.9964 | ValAcc 0.9968 | dice 0.8686 | f1 0.9165 | recall 0.8224 | jaccard 0.7683 | hausdorff 5.5516 | usr 0.0942 | osr 0.0373 | BestDice 0.8811 | Pat 2 | 24.4s\n",
      "Epoch 023 | TrainLoss 0.0279 | ValLoss 0.0257 | TrainAcc 0.9964 | ValAcc 0.9970 | dice 0.8848 | f1 0.9264 | recall 0.8832 | jaccard 0.7939 | hausdorff 4.1683 | usr 0.0589 | osr 0.0564 | BestDice 0.8848 | Pat 0 | 24.4s\n",
      "Epoch 024 | TrainLoss 0.0265 | ValLoss 0.0255 | TrainAcc 0.9965 | ValAcc 0.9966 | dice 0.8706 | f1 0.9178 | recall 0.8912 | jaccard 0.7717 | hausdorff 6.4773 | usr 0.0535 | osr 0.0760 | BestDice 0.8848 | Pat 1 | 24.5s\n",
      "Epoch 025 | TrainLoss 0.0254 | ValLoss 0.0238 | TrainAcc 0.9964 | ValAcc 0.9969 | dice 0.8811 | f1 0.9241 | recall 0.8972 | jaccard 0.7881 | hausdorff 4.2415 | usr 0.0509 | osr 0.0682 | BestDice 0.8848 | Pat 2 | 24.4s\n",
      "Epoch 026 | TrainLoss 0.0244 | ValLoss 0.0228 | TrainAcc 0.9964 | ValAcc 0.9968 | dice 0.8751 | f1 0.9205 | recall 0.8541 | jaccard 0.7786 | hausdorff 5.5473 | usr 0.0751 | osr 0.0499 | BestDice 0.8848 | Pat 3 | 24.4s\n",
      "Epoch 027 | TrainLoss 0.0231 | ValLoss 0.0217 | TrainAcc 0.9965 | ValAcc 0.9969 | dice 0.8769 | f1 0.9216 | recall 0.8626 | jaccard 0.7815 | hausdorff 4.2395 | usr 0.0703 | osr 0.0529 | BestDice 0.8848 | Pat 4 | 24.5s\n",
      "Epoch 028 | TrainLoss 0.0220 | ValLoss 0.0206 | TrainAcc 0.9966 | ValAcc 0.9970 | dice 0.8843 | f1 0.9261 | recall 0.8943 | jaccard 0.7933 | hausdorff 3.5205 | usr 0.0526 | osr 0.0631 | BestDice 0.8848 | Pat 5 | 24.4s\n",
      "Epoch 029 | TrainLoss 0.0213 | ValLoss 0.0201 | TrainAcc 0.9966 | ValAcc 0.9969 | dice 0.8742 | f1 0.9199 | recall 0.8268 | jaccard 0.7773 | hausdorff 4.6642 | usr 0.0921 | osr 0.0338 | BestDice 0.8848 | Pat 6 | 24.5s\n",
      "Epoch 030 | TrainLoss 0.0208 | ValLoss 0.0231 | TrainAcc 0.9965 | ValAcc 0.9961 | dice 0.8476 | f1 0.9043 | recall 0.8463 | jaccard 0.7369 | hausdorff 12.0478 | usr 0.0774 | osr 0.0751 | BestDice 0.8848 | Pat 7 | 24.2s\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "#NP training\n",
    "history_csv = OUTPUT_ROOT / \"history.csv\"\n",
    "best_model_path = OUTPUT_ROOT / f\"best_model_{CONFIG['model_tag']}.pth\"\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"Detected existing model {best_model_path.name}. Skip training (delete it to retrain).\")\n",
    "else:\n",
    "    best_dice = -1\n",
    "    epochs_no_improve = 0\n",
    "    metric_names = list(METRIC_FUNCS.keys())\n",
    "    display_metrics = [\"dice\",\"f1\",\"recall\",\"jaccard\",\"hausdorff\",\"usr\",\"osr\"]  # keeping original order\n",
    "\n",
    "    for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc_sum = 0.0  # added\n",
    "        start_time = time.time()\n",
    "        for imgs, lbls in train_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=CONFIG[\"mixed_precision\"]):\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, lbls)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            # accumulate loss\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            # accumulate training accuracy (thresholded at 0.5 inside calculate_accuracy)\n",
    "            with torch.no_grad():\n",
    "                probs_tr = torch.sigmoid(logits)\n",
    "                batch_acc = calculate_accuracy(lbls, probs_tr)\n",
    "            train_acc_sum += batch_acc.item() * imgs.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_acc_sum / len(train_loader.dataset)  # added\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        agg = MetricAggregator()\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in val_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, lbls)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                agg.update(lbls, probs)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_metrics = agg.compute()\n",
    "        epoch_dice = val_metrics[\"dice\"]\n",
    "        val_acc = val_metrics[\"accuracy\"]  # added\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        is_best = epoch_dice > best_dice\n",
    "        if is_best:\n",
    "            best_dice = epoch_dice\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        save_checkpoint({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"val_dice\": epoch_dice,\n",
    "            \"config\": CONFIG\n",
    "        }, is_best=is_best)\n",
    "\n",
    "        row = [epoch, train_loss, val_loss, *[val_metrics[m] for m in metric_names], best_dice, elapsed, train_acc, val_acc]\n",
    "        header = [\"epoch\",\"train_loss\",\"val_loss\", *metric_names,\"best_dice\",\"seconds\",\"train_accuracy\",\"val_accuracy\"]\n",
    "        save_history(row, header, history_csv)\n",
    "\n",
    "        disp = \" | \".join([f\"{m} {val_metrics[m]:.4f}\" for m in display_metrics])\n",
    "        print(f\"Epoch {epoch:03d} | TrainLoss {train_loss:.4f} | ValLoss {val_loss:.4f} | \"\n",
    "              f\"TrainAcc {train_acc:.4f} | ValAcc {val_acc:.4f} | {disp} | \"\n",
    "              f\"BestDice {best_dice:.4f} | Pat {epochs_no_improve} | {elapsed:.1f}s\")\n",
    "\n",
    "        if epochs_no_improve >= CONFIG[\"early_stopping_patience\"]:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d124c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model (best_model_NP.pth) from epoch 23 (val_dice=0.8848)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:25: DeprecationWarning: Please import `generate_binary_structure` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  conn = morphology.generate_binary_structure(y_true_np.ndim, 1)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:26: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_surface = np.logical_xor(y_true_np, morphology.binary_erosion(y_true_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:27: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_surface = np.logical_xor(y_pred_np, morphology.binary_erosion(y_pred_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:30: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_distances = morphology.distance_transform_edt(~true_surface, sampling=spacing)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:31: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_distances = morphology.distance_transform_edt(~pred_surface, sampling=spacing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics (best model)\n",
      "split      | dice   | f1     | recall | jaccard | hausdorff | usr    | osr    | asd     | accuracy | mean_iou | loss  \n",
      "-----------+--------+--------+--------+---------+-----------+--------+--------+---------+----------+----------+-------\n",
      "validation | 0.8848 | 0.9264 | 0.8832 | 0.7939  | 4.1683    | 0.0589 | 0.0564 | 22.1161 | 0.9970   | 0.7939   | 0.0257\n",
      "test       | 0.8833 | 0.9254 | 0.8948 | 0.7914  | 4.3425    | 0.0523 | 0.0645 | 22.4111 | 0.9972   | 0.7914   | 0.0254\n",
      "\n",
      "Saved evaluation summary to VNETModels\\HARP_NP\\eval_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate best saved model on validation and test sets (prints metrics table) ---\n",
    "from collections import OrderedDict\n",
    "\n",
    "def evaluate_loader(model, loader, criterion, metric_funcs):\n",
    "    model.eval()\n",
    "    agg = MetricAggregator(metric_funcs)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, lbls)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            agg.update(lbls, probs)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    metrics = agg.compute()\n",
    "    metrics = OrderedDict(metrics)  # preserve order\n",
    "    metrics[\"loss\"] = avg_loss\n",
    "    return metrics\n",
    "\n",
    "best_model_path = OUTPUT_ROOT / f\"best_model_{CONFIG['model_tag']}.pth\"\n",
    "assert best_model_path.exists(), f\"Best model not found: {best_model_path}\"\n",
    "\n",
    "ckpt = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "print(f\"Loaded best model ({best_model_path.name}) from epoch {ckpt.get('epoch')} (val_dice={ckpt.get('val_dice'):.4f})\")\n",
    "\n",
    "metric_names = list(METRIC_FUNCS.keys())  # ordered as defined\n",
    "val_metrics  = evaluate_loader(model, val_loader,  criterion, METRIC_FUNCS)\n",
    "test_metrics = evaluate_loader(model, test_loader, criterion, METRIC_FUNCS)\n",
    "\n",
    "# Build table rows (ensure same column order)\n",
    "columns = metric_names + [\"loss\"]\n",
    "def row_str(split, metrics):\n",
    "    return [split] + [f\"{metrics[m]:.4f}\" for m in metric_names] + [f\"{metrics['loss']:.4f}\"]\n",
    "\n",
    "header = [\"split\"] + columns\n",
    "rows = [\n",
    "    row_str(\"validation\", val_metrics),\n",
    "    row_str(\"test\",       test_metrics),\n",
    "]\n",
    "\n",
    "# Pretty print table\n",
    "col_widths = [max(len(header[i]), *(len(r[i]) for r in rows)) for i in range(len(header))]\n",
    "def fmt_line(cells):\n",
    "    return \" | \".join(c.ljust(col_widths[i]) for i,c in enumerate(cells))\n",
    "sep = \"-+-\".join(\"-\"*w for w in col_widths)\n",
    "\n",
    "print(\"\\nEvaluation Metrics (best model)\")\n",
    "print(fmt_line(header))\n",
    "print(sep)\n",
    "for r in rows:\n",
    "    print(fmt_line(r))\n",
    "\n",
    "# Optionally save to CSV\n",
    "summary_csv = OUTPUT_ROOT / \"eval_summary.csv\"\n",
    "import csv\n",
    "write_header = not summary_csv.exists()\n",
    "with open(summary_csv, \"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if write_header:\n",
    "        w.writerow(header)\n",
    "    for r in rows:\n",
    "        w.writerow(r)\n",
    "\n",
    "print(f\"\\nSaved evaluation summary to {summary_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3d72c",
   "metadata": {},
   "source": [
    "##### training, testing (dataset key = HARP_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe271e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP Train batches: 55 Val: 13 Test: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:25: DeprecationWarning: Please import `generate_binary_structure` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  conn = morphology.generate_binary_structure(y_true_np.ndim, 1)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:26: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_surface = np.logical_xor(y_true_np, morphology.binary_erosion(y_true_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:27: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_surface = np.logical_xor(y_pred_np, morphology.binary_erosion(y_pred_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:30: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_distances = morphology.distance_transform_edt(~true_surface, sampling=spacing)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:31: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_distances = morphology.distance_transform_edt(~pred_surface, sampling=spacing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP] Epoch 001 | TrainLoss 0.5775 | ValLoss 0.4790 | TrainAcc 0.8092 | ValAcc 0.9727 | dice 0.4688 | f1 0.7065 | recall 0.9223 | jaccard 0.3077 | hausdorff 41.0587 | usr 0.0205 | osr 0.5108 | BestDice 0.4688 | Pat 0 | 25.9s\n",
      "[FP] Epoch 002 | TrainLoss 0.4263 | ValLoss 0.3798 | TrainAcc 0.9856 | ValAcc 0.9923 | dice 0.7294 | f1 0.8398 | recall 0.7988 | jaccard 0.5775 | hausdorff 9.2206 | usr 0.0920 | osr 0.1786 | BestDice 0.7294 | Pat 0 | 18.6s\n",
      "[FP] Epoch 003 | TrainLoss 0.3430 | ValLoss 0.3066 | TrainAcc 0.9910 | ValAcc 0.9920 | dice 0.7336 | f1 0.8405 | recall 0.8489 | jaccard 0.5812 | hausdorff 8.3706 | usr 0.0667 | osr 0.1997 | BestDice 0.7336 | Pat 0 | 18.6s\n",
      "[FP] Epoch 004 | TrainLoss 0.2728 | ValLoss 0.2394 | TrainAcc 0.9922 | ValAcc 0.9942 | dice 0.7775 | f1 0.8648 | recall 0.7780 | jaccard 0.6376 | hausdorff 7.5145 | usr 0.1119 | osr 0.1107 | BestDice 0.7775 | Pat 0 | 21.5s\n",
      "[FP] Epoch 005 | TrainLoss 0.2141 | ValLoss 0.1890 | TrainAcc 0.9933 | ValAcc 0.9947 | dice 0.8033 | f1 0.8788 | recall 0.8439 | jaccard 0.6729 | hausdorff 6.6312 | usr 0.0749 | osr 0.1219 | BestDice 0.8033 | Pat 0 | 24.4s\n",
      "[FP] Epoch 006 | TrainLoss 0.1702 | ValLoss 0.1520 | TrainAcc 0.9941 | ValAcc 0.9945 | dice 0.8001 | f1 0.8770 | recall 0.8566 | jaccard 0.6688 | hausdorff 6.1308 | usr 0.0676 | osr 0.1325 | BestDice 0.8033 | Pat 1 | 24.4s\n",
      "[FP] Epoch 007 | TrainLoss 0.1386 | ValLoss 0.1247 | TrainAcc 0.9946 | ValAcc 0.9956 | dice 0.8332 | f1 0.8959 | recall 0.8429 | jaccard 0.7154 | hausdorff 6.8764 | usr 0.0781 | osr 0.0888 | BestDice 0.8332 | Pat 0 | 24.5s\n",
      "[FP] Epoch 008 | TrainLoss 0.1144 | ValLoss 0.1027 | TrainAcc 0.9949 | ValAcc 0.9952 | dice 0.8293 | f1 0.8933 | recall 0.9147 | jaccard 0.7100 | hausdorff 4.6858 | usr 0.0392 | osr 0.1316 | BestDice 0.8332 | Pat 1 | 24.4s\n",
      "[FP] Epoch 009 | TrainLoss 0.0947 | ValLoss 0.0859 | TrainAcc 0.9953 | ValAcc 0.9959 | dice 0.8460 | f1 0.9032 | recall 0.8609 | jaccard 0.7339 | hausdorff 5.3986 | usr 0.0689 | osr 0.0853 | BestDice 0.8460 | Pat 0 | 24.4s\n",
      "[FP] Epoch 010 | TrainLoss 0.0809 | ValLoss 0.0747 | TrainAcc 0.9953 | ValAcc 0.9954 | dice 0.8322 | f1 0.8952 | recall 0.8887 | jaccard 0.7143 | hausdorff 5.8829 | usr 0.0525 | osr 0.1154 | BestDice 0.8460 | Pat 1 | 24.4s\n",
      "[FP] Epoch 011 | TrainLoss 0.0709 | ValLoss 0.0647 | TrainAcc 0.9952 | ValAcc 0.9962 | dice 0.8543 | f1 0.9082 | recall 0.8603 | jaccard 0.7466 | hausdorff 4.9552 | usr 0.0699 | osr 0.0759 | BestDice 0.8543 | Pat 0 | 24.5s\n",
      "[FP] Epoch 012 | TrainLoss 0.0621 | ValLoss 0.0573 | TrainAcc 0.9956 | ValAcc 0.9963 | dice 0.8545 | f1 0.9082 | recall 0.8303 | jaccard 0.7467 | hausdorff 5.8023 | usr 0.0876 | osr 0.0580 | BestDice 0.8545 | Pat 0 | 24.4s\n",
      "[FP] Epoch 013 | TrainLoss 0.0555 | ValLoss 0.0522 | TrainAcc 0.9957 | ValAcc 0.9960 | dice 0.8423 | f1 0.9010 | recall 0.8181 | jaccard 0.7283 | hausdorff 7.1782 | usr 0.0940 | osr 0.0638 | BestDice 0.8545 | Pat 1 | 24.4s\n",
      "[FP] Epoch 014 | TrainLoss 0.0501 | ValLoss 0.0469 | TrainAcc 0.9958 | ValAcc 0.9961 | dice 0.8538 | f1 0.9078 | recall 0.8878 | jaccard 0.7458 | hausdorff 4.6588 | usr 0.0543 | osr 0.0920 | BestDice 0.8545 | Pat 2 | 24.3s\n",
      "[FP] Epoch 015 | TrainLoss 0.0458 | ValLoss 0.0436 | TrainAcc 0.9958 | ValAcc 0.9959 | dice 0.8418 | f1 0.9008 | recall 0.8345 | jaccard 0.7276 | hausdorff 5.8576 | usr 0.0839 | osr 0.0745 | BestDice 0.8545 | Pat 3 | 24.4s\n",
      "[FP] Epoch 016 | TrainLoss 0.0424 | ValLoss 0.0415 | TrainAcc 0.9958 | ValAcc 0.9953 | dice 0.8280 | f1 0.8928 | recall 0.8684 | jaccard 0.7077 | hausdorff 8.6611 | usr 0.0631 | osr 0.1091 | BestDice 0.8545 | Pat 4 | 24.4s\n",
      "[FP] Epoch 017 | TrainLoss 0.0394 | ValLoss 0.0367 | TrainAcc 0.9957 | ValAcc 0.9963 | dice 0.8561 | f1 0.9092 | recall 0.8659 | jaccard 0.7493 | hausdorff 4.5733 | usr 0.0666 | osr 0.0774 | BestDice 0.8561 | Pat 0 | 24.4s\n",
      "[FP] Epoch 018 | TrainLoss 0.0362 | ValLoss 0.0342 | TrainAcc 0.9960 | ValAcc 0.9964 | dice 0.8558 | f1 0.9089 | recall 0.8332 | jaccard 0.7485 | hausdorff 5.0420 | usr 0.0860 | osr 0.0582 | BestDice 0.8561 | Pat 1 | 24.3s\n",
      "[FP] Epoch 019 | TrainLoss 0.0338 | ValLoss 0.0317 | TrainAcc 0.9960 | ValAcc 0.9964 | dice 0.8618 | f1 0.9125 | recall 0.8713 | jaccard 0.7578 | hausdorff 4.5100 | usr 0.0640 | osr 0.0743 | BestDice 0.8618 | Pat 0 | 24.5s\n",
      "[FP] Epoch 020 | TrainLoss 0.0314 | ValLoss 0.0295 | TrainAcc 0.9962 | ValAcc 0.9966 | dice 0.8696 | f1 0.9172 | recall 0.8875 | jaccard 0.7702 | hausdorff 4.3783 | usr 0.0555 | osr 0.0750 | BestDice 0.8696 | Pat 0 | 24.4s\n",
      "[FP] Epoch 021 | TrainLoss 0.0297 | ValLoss 0.0288 | TrainAcc 0.9962 | ValAcc 0.9961 | dice 0.8520 | f1 0.9069 | recall 0.8819 | jaccard 0.7437 | hausdorff 5.0336 | usr 0.0573 | osr 0.0908 | BestDice 0.8696 | Pat 1 | 24.5s\n",
      "[FP] Epoch 022 | TrainLoss 0.0282 | ValLoss 0.0272 | TrainAcc 0.9962 | ValAcc 0.9964 | dice 0.8530 | f1 0.9072 | recall 0.8119 | jaccard 0.7443 | hausdorff 6.3482 | usr 0.0992 | osr 0.0479 | BestDice 0.8696 | Pat 2 | 24.4s\n",
      "[FP] Epoch 023 | TrainLoss 0.0272 | ValLoss 0.0253 | TrainAcc 0.9960 | ValAcc 0.9965 | dice 0.8622 | f1 0.9128 | recall 0.8474 | jaccard 0.7584 | hausdorff 5.0436 | usr 0.0780 | osr 0.0600 | BestDice 0.8696 | Pat 3 | 24.5s\n",
      "[FP] Epoch 024 | TrainLoss 0.0255 | ValLoss 0.0247 | TrainAcc 0.9962 | ValAcc 0.9963 | dice 0.8508 | f1 0.9060 | recall 0.8224 | jaccard 0.7412 | hausdorff 5.6093 | usr 0.0921 | osr 0.0573 | BestDice 0.8696 | Pat 4 | 24.6s\n",
      "[FP] Epoch 025 | TrainLoss 0.0243 | ValLoss 0.0232 | TrainAcc 0.9963 | ValAcc 0.9965 | dice 0.8657 | f1 0.9149 | recall 0.8798 | jaccard 0.7642 | hausdorff 4.5825 | usr 0.0595 | osr 0.0749 | BestDice 0.8696 | Pat 5 | 24.5s\n",
      "[FP] Epoch 026 | TrainLoss 0.0230 | ValLoss 0.0220 | TrainAcc 0.9964 | ValAcc 0.9966 | dice 0.8674 | f1 0.9159 | recall 0.8583 | jaccard 0.7665 | hausdorff 4.9940 | usr 0.0720 | osr 0.0607 | BestDice 0.8696 | Pat 6 | 24.4s\n",
      "[FP] Epoch 027 | TrainLoss 0.0224 | ValLoss 0.0216 | TrainAcc 0.9963 | ValAcc 0.9964 | dice 0.8596 | f1 0.9113 | recall 0.8646 | jaccard 0.7548 | hausdorff 5.4610 | usr 0.0677 | osr 0.0728 | BestDice 0.8696 | Pat 7 | 24.5s\n",
      "[FP] Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# --- FP dataset training & evaluation \n",
    "from pathlib import Path\n",
    "# Configure FP run\n",
    "DATASET_KEY_FP = \"HARP_FP\"\n",
    "MODEL_TAG_FP = \"FP\"\n",
    "CONFIG_FP = {\n",
    "    \"dataset\": DATASET_KEY_FP,\n",
    "    \"model_tag\": MODEL_TAG_FP,\n",
    "    \"model_name\": f\"VNet_{MODEL_TAG_FP}\",\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 250,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"loss\": \"BCE\",\n",
    "    \"threshold\": 0.5,\n",
    "    \"early_stopping_patience\": 7,\n",
    "    \"mixed_precision\": True\n",
    "}\n",
    "\n",
    "OUTPUT_ROOT_FP = Path(\"VNETModels\") / DATASET_KEY_FP\n",
    "OUTPUT_ROOT_FP.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_ROOT_FP / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "with open(OUTPUT_ROOT_FP / \"config.json\", \"w\") as f:\n",
    "    import json; json.dump(CONFIG_FP, f, indent=2)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader_fp, val_loader_fp, test_loader_fp = get_dataloaders(\n",
    "    CONFIG_FP[\"dataset\"],\n",
    "    batch_size=CONFIG_FP[\"batch_size\"]\n",
    ")\n",
    "\n",
    "# Model / optim / loss\n",
    "model_fp = VNet(in_channels=1, out_channels=1).to(device)\n",
    "criterion_fp = get_loss(CONFIG_FP[\"loss\"])\n",
    "optimizer_fp = torch.optim.Adam(model_fp.parameters(),\n",
    "                                lr=CONFIG_FP[\"lr\"],\n",
    "                                weight_decay=CONFIG_FP[\"weight_decay\"])\n",
    "scaler_fp = GradScaler(enabled=CONFIG_FP[\"mixed_precision\"])\n",
    "\n",
    "print(\"FP Train batches:\", len(train_loader_fp),\n",
    "      \"Val:\", len(val_loader_fp),\n",
    "      \"Test:\", len(test_loader_fp))\n",
    "\n",
    "# Checkpoint helper (only best)\n",
    "def save_checkpoint_fp(state, is_best=False):\n",
    "    if not is_best:\n",
    "        return\n",
    "    best_path = OUTPUT_ROOT_FP / f\"best_model_{CONFIG_FP['model_tag']}.pth\"\n",
    "    torch.save(state, best_path)\n",
    "    (OUTPUT_ROOT_FP / f\"trained_{CONFIG_FP['model_tag']}.flag\").write_text(str(state[\"epoch\"]))\n",
    "\n",
    "# History helper\n",
    "def save_history_fp(history_rows, header, path):\n",
    "    import csv\n",
    "    write_header = not path.exists()\n",
    "    with open(path, \"a\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow(header)\n",
    "        w.writerow(history_rows)\n",
    "\n",
    "history_csv_fp = OUTPUT_ROOT_FP / \"history.csv\"\n",
    "best_model_path_fp = OUTPUT_ROOT_FP / f\"best_model_{CONFIG_FP['model_tag']}.pth\"\n",
    "\n",
    "# Training\n",
    "if best_model_path_fp.exists():\n",
    "    print(f\"Detected existing FP model {best_model_path_fp.name}. Skip training (delete it to retrain).\")\n",
    "else:\n",
    "    best_dice_fp = -1\n",
    "    epochs_no_improve_fp = 0\n",
    "    metric_names_fp = list(METRIC_FUNCS.keys())\n",
    "    display_metrics_fp = [\"dice\",\"f1\",\"recall\",\"jaccard\",\"hausdorff\",\"usr\",\"osr\"]\n",
    "\n",
    "    for epoch in range(1, CONFIG_FP[\"epochs\"] + 1):\n",
    "        model_fp.train()\n",
    "        train_loss_fp = 0.0\n",
    "        train_acc_sum_fp = 0.0\n",
    "        start_time_fp = time.time()\n",
    "\n",
    "        for imgs, lbls in train_loader_fp:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            optimizer_fp.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=CONFIG_FP[\"mixed_precision\"]):\n",
    "                logits_fp = model_fp(imgs)\n",
    "                loss_fp = criterion_fp(logits_fp, lbls)\n",
    "            scaler_fp.scale(loss_fp).backward()\n",
    "            scaler_fp.step(optimizer_fp)\n",
    "            scaler_fp.update()\n",
    "\n",
    "            train_loss_fp += loss_fp.item() * imgs.size(0)\n",
    "            with torch.no_grad():\n",
    "                probs_tr_fp = torch.sigmoid(logits_fp)\n",
    "                batch_acc_fp = calculate_accuracy(lbls, probs_tr_fp)\n",
    "            train_acc_sum_fp += batch_acc_fp.item() * imgs.size(0)\n",
    "\n",
    "        train_loss_fp /= len(train_loader_fp.dataset)\n",
    "        train_acc_fp = train_acc_sum_fp / len(train_loader_fp.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model_fp.eval()\n",
    "        val_loss_fp = 0.0\n",
    "        agg_fp = MetricAggregator()\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in val_loader_fp:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                logits_fp = model_fp(imgs)\n",
    "                loss_fp = criterion_fp(logits_fp, lbls)\n",
    "                val_loss_fp += loss_fp.item() * imgs.size(0)\n",
    "                probs_fp = torch.sigmoid(logits_fp)\n",
    "                agg_fp.update(lbls, probs_fp)\n",
    "\n",
    "        val_loss_fp /= len(val_loader_fp.dataset)\n",
    "        val_metrics_fp = agg_fp.compute()\n",
    "        epoch_dice_fp = val_metrics_fp[\"dice\"]\n",
    "        val_acc_fp = val_metrics_fp[\"accuracy\"]\n",
    "\n",
    "        elapsed_fp = time.time() - start_time_fp\n",
    "        is_best_fp = epoch_dice_fp > best_dice_fp\n",
    "        if is_best_fp:\n",
    "            best_dice_fp = epoch_dice_fp\n",
    "            epochs_no_improve_fp = 0\n",
    "        else:\n",
    "            epochs_no_improve_fp += 1\n",
    "\n",
    "        save_checkpoint_fp({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model_fp.state_dict(),\n",
    "            \"optimizer_state\": optimizer_fp.state_dict(),\n",
    "            \"val_dice\": epoch_dice_fp,\n",
    "            \"config\": CONFIG_FP\n",
    "        }, is_best=is_best_fp)\n",
    "\n",
    "        row_fp = [epoch, train_loss_fp, val_loss_fp,\n",
    "                  *[val_metrics_fp[m] for m in metric_names_fp],\n",
    "                  best_dice_fp, elapsed_fp, train_acc_fp, val_acc_fp]\n",
    "        header_fp = [\"epoch\",\"train_loss\",\"val_loss\", *metric_names_fp,\n",
    "                     \"best_dice\",\"seconds\",\"train_accuracy\",\"val_accuracy\"]\n",
    "        save_history_fp(row_fp, header_fp, history_csv_fp)\n",
    "\n",
    "        disp_fp = \" | \".join([f\"{m} {val_metrics_fp[m]:.4f}\" for m in display_metrics_fp])\n",
    "        print(f\"[FP] Epoch {epoch:03d} | TrainLoss {train_loss_fp:.4f} | ValLoss {val_loss_fp:.4f} | \"\n",
    "              f\"TrainAcc {train_acc_fp:.4f} | ValAcc {val_acc_fp:.4f} | {disp_fp} | \"\n",
    "              f\"BestDice {best_dice_fp:.4f} | Pat {epochs_no_improve_fp} | {elapsed_fp:.1f}s\")\n",
    "\n",
    "        if epochs_no_improve_fp >= CONFIG_FP[\"early_stopping_patience\"]:\n",
    "            print(\"[FP] Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a189b1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FP] Loaded best model epoch 20 (val_dice=0.8696)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:25: DeprecationWarning: Please import `generate_binary_structure` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  conn = morphology.generate_binary_structure(y_true_np.ndim, 1)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:26: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_surface = np.logical_xor(y_true_np, morphology.binary_erosion(y_true_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:27: DeprecationWarning: Please import `binary_erosion` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_surface = np.logical_xor(y_pred_np, morphology.binary_erosion(y_pred_np, conn))\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:30: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  true_distances = morphology.distance_transform_edt(~true_surface, sampling=spacing)\n",
      "C:\\Users\\Faizaan\\AppData\\Local\\Temp\\ipykernel_19796\\2610207449.py:31: DeprecationWarning: Please import `distance_transform_edt` from the `scipy.ndimage` namespace; the `scipy.ndimage.morphology` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  pred_distances = morphology.distance_transform_edt(~pred_surface, sampling=spacing)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FP] Evaluation Metrics (best model)\n",
      "split         | dice   | f1     | recall | jaccard | hausdorff | usr    | osr    | asd     | accuracy | mean_iou | loss  \n",
      "--------------+--------+--------+--------+---------+-----------+--------+--------+---------+----------+----------+-------\n",
      "validation_FP | 0.8696 | 0.9172 | 0.8875 | 0.7702  | 4.3783    | 0.0555 | 0.0750 | 22.1161 | 0.9966   | 0.7702   | 0.0295\n",
      "test_FP       | 0.8683 | 0.9163 | 0.8944 | 0.7676  | 4.6690    | 0.0515 | 0.0803 | 22.4111 | 0.9968   | 0.7676   | 0.0291\n",
      "\n",
      "[FP] Saved evaluation summary to VNETModels\\HARP_FP\\eval_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Evaluation (FP) ---\n",
    "from collections import OrderedDict\n",
    "\n",
    "def evaluate_loader_fp(model_eval, loader, criterion, metric_funcs):\n",
    "    model_eval.eval()\n",
    "    agg_eval = MetricAggregator(metric_funcs)\n",
    "    total_loss_eval = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model_eval(imgs)\n",
    "            loss = criterion(logits, lbls)\n",
    "            total_loss_eval += loss.item() * imgs.size(0)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            agg_eval.update(lbls, probs)\n",
    "    avg_loss_eval = total_loss_eval / len(loader.dataset)\n",
    "    metrics_eval = agg_eval.compute()\n",
    "    metrics_eval = OrderedDict(metrics_eval)\n",
    "    metrics_eval[\"loss\"] = avg_loss_eval\n",
    "    return metrics_eval\n",
    "\n",
    "assert best_model_path_fp.exists(), \"Best FP model not found (train stage may have been skipped unexpectedly).\"\n",
    "ckpt_fp = torch.load(best_model_path_fp, map_location=device)\n",
    "model_fp.load_state_dict(ckpt_fp[\"model_state\"])\n",
    "print(f\"\\n[FP] Loaded best model epoch {ckpt_fp.get('epoch')} (val_dice={ckpt_fp.get('val_dice'):.4f})\")\n",
    "\n",
    "metric_names_fp = list(METRIC_FUNCS.keys())\n",
    "val_metrics_fp  = evaluate_loader_fp(model_fp, val_loader_fp,  criterion_fp, METRIC_FUNCS)\n",
    "test_metrics_fp = evaluate_loader_fp(model_fp, test_loader_fp, criterion_fp, METRIC_FUNCS)\n",
    "\n",
    "columns_fp = metric_names_fp + [\"loss\"]\n",
    "def row_fp_str(split, mets):\n",
    "    return [split] + [f\"{mets[m]:.4f}\" for m in metric_names_fp] + [f\"{mets['loss']:.4f}\"]\n",
    "\n",
    "header_fp_tbl = [\"split\"] + columns_fp\n",
    "rows_fp = [\n",
    "    row_fp_str(\"validation_FP\", val_metrics_fp),\n",
    "    row_fp_str(\"test_FP\",       test_metrics_fp),\n",
    "]\n",
    "\n",
    "widths_fp = [max(len(header_fp_tbl[i]), *(len(r[i]) for r in rows_fp)) for i in range(len(header_fp_tbl))]\n",
    "def fmt_fp(cells): return \" | \".join(c.ljust(widths_fp[i]) for i,c in enumerate(cells))\n",
    "sep_fp = \"-+-\".join(\"-\"*w for w in widths_fp)\n",
    "\n",
    "print(\"\\n[FP] Evaluation Metrics (best model)\")\n",
    "print(fmt_fp(header_fp_tbl))\n",
    "print(sep_fp)\n",
    "for r in rows_fp:\n",
    "    print(fmt_fp(r))\n",
    "\n",
    "# Save summary\n",
    "summary_fp_csv = OUTPUT_ROOT_FP / \"eval_summary.csv\"\n",
    "import csv\n",
    "write_header_fp = not summary_fp_csv.exists()\n",
    "with open(summary_fp_csv, \"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if write_header_fp:\n",
    "        w.writerow(header_fp_tbl)\n",
    "    for r in rows_fp:\n",
    "        w.writerow(r)\n",
    "print(f\"\\n[FP] Saved evaluation summary to {summary_fp_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
